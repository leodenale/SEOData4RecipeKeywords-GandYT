{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "SEOData4RecipeKeyword.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leodenale/SEOData4RecipeKeywords-GandYT/blob/master/SEOData4RecipeKeyword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzwScvRitYBp"
      },
      "source": [
        "# SEO Data for Recipe Keywords (Google & YouTube)\n",
        "## 484 queries' results on Google and YouTube, with metadata\n",
        "\n",
        "I've written previously on search data and what can be done with it for different industries:\n",
        "\n",
        "- SEMrush article [analyzing tickets and flights keywords on Google search](https://www.semrush.com/blog/analyzing-search-engine-results-pages/)\n",
        "- [Cars for sale on Google search](https://www.kaggle.com/eliasdabbas/search-engine-results-pages-serps-research) (explains more on how to use the [`serp_goog` function](https://advertools.readthedocs.io/en/master/advertools.html#module-advertools.serp))\n",
        "- [US 2018 Midterm Elections candidates on Google Search](https://www.kaggle.com/eliasdabbas/us-midterm-elections-2018-on-google-search)\n",
        "- You can also generate your own similar dataset through this [SEO keyword SERP dashboard](https://www.dashboardom.com/google-serp) if you want\n",
        "\n",
        "Here, I go through the same, but for a different industry (recipes and food), and I also include data from YouTube search results.  \n",
        "YouTube provides much richer data about the search results, mainly because it hosts the content and the engagement on the platform. On regular search, you basically have one metric, which is the rank of the keyword, but on YouTube, you have a lot of data on views, comments, likes, etc. for the video, as well as a lot of metadata about the channel that produced the video; the number of videos they have, subscribers, total views, and so on. \n",
        "\n",
        "I'll be using `pandas` for data manipulation, `plotly` for data visualization, and `advertools`, for importing search data (and having it in a DataFrame, analyzing the text (titles, descriptions, etc.), and how it relates to numbers (views, likes, subscribers, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2XTePPUvitK",
        "outputId": "99583b98-17b2-4729-e0aa-28396bc033c7"
      },
      "source": [
        "pip install advertools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting advertools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/08/a57de3ef9acced997ac6397d9aa7c4a597c3cb59f61dc2b90a72fcd99396/advertools-0.10.7-py2.py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 5.5MB/s \n",
            "\u001b[?25hCollecting twython\n",
            "  Downloading https://files.pythonhosted.org/packages/24/80/579b96dfaa9b536efde883d4f0df7ea2598a6f3117a6dd572787f4a2bcfb/twython-3.8.2-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from advertools) (1.1.5)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from advertools) (0.4.8)\n",
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/16/3c7c37caf25f91aa21db194655515718c2a15f704f9f5c59a194f5c83db0/Scrapy-2.4.1-py2.py3-none-any.whl (239kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from twython->advertools) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from twython->advertools) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->advertools) (1.19.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->advertools) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->advertools) (2018.9)\n",
            "Collecting service-identity>=16.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting cssselect>=0.9.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting cryptography>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/de/7054df0620b5411ba45480f0261e1fb66a53f3db31b28e3aa52c026e72d9/cryptography-3.3.1-cp36-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 8.5MB/s \n",
            "\u001b[?25hCollecting parsel>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from scrapy->advertools) (4.2.6)\n",
            "Collecting protego>=0.1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 25.1MB/s \n",
            "\u001b[?25hCollecting queuelib>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting itemloaders>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/2b/eb2ddf7becf834679273a6f79ffdc6fbedf07c5272e2eddf412582143c0e/itemloaders-1.0.4-py3-none-any.whl\n",
            "Collecting zope.interface>=4.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 32.6MB/s \n",
            "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting Twisted>=17.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 37.3MB/s \n",
            "\u001b[?25hCollecting itemadapter>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/88/83/ab33780fd93278e699561d61862d27343c95d3fe0a0081acd73e8e26a649/itemadapter-0.2.0-py3-none-any.whl\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/5e/06351ede29fd4899782ad335c2e02f1f862a887c20a3541f17c3fa1a3525/pyOpenSSL-20.0.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython->advertools) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython->advertools) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython->advertools) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython->advertools) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.4.0->twython->advertools) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->advertools) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy->advertools) (0.2.8)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy->advertools) (20.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy->advertools) (1.14.4)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy->advertools) (51.1.1)\n",
            "Collecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/aa/8caf6a0a3e62863cbb9dab27135660acba46903b703e224f14f447e57934/hyperlink-21.0.0-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.6MB/s \n",
            "\u001b[?25hCollecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n",
            "\u001b[?25hCollecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy->advertools) (2.20)\n",
            "Building wheels for collected packages: protego, PyDispatcher\n",
            "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7766 sha256=6208da19621ff39bb1f9bf60532686599c1a38b6279ecf56b4faeb74bfe513a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11516 sha256=04794c882e3c97a91d543b5d2c307c910cabab851dc407d9f39c485c62012d2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built protego PyDispatcher\n",
            "Installing collected packages: twython, cryptography, service-identity, w3lib, cssselect, parsel, protego, queuelib, jmespath, itemadapter, itemloaders, zope.interface, PyDispatcher, constantly, hyperlink, Automat, PyHamcrest, incremental, Twisted, pyOpenSSL, scrapy, advertools\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 advertools-0.10.7 constantly-15.1.0 cryptography-3.3.1 cssselect-1.1.0 hyperlink-21.0.0 incremental-17.5.0 itemadapter-0.2.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 protego-0.1.16 pyOpenSSL-20.0.1 queuelib-1.5.0 scrapy-2.4.1 service-identity-18.1.0 twython-3.8.2 w3lib-1.22.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "LoEfdYXBtYB0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "ee644c4e-5288-4a51-becf-b361880687c5"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_columns = None\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot, plot\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "import advertools as adv\n",
        "print('Package        Version')\n",
        "print('='*25)\n",
        "for p in [adv, pd, plotly]:\n",
        "    print(f\"{p.__name__:<15}\", ': ', p.__version__, sep='')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Package        Version\n",
            "=========================\n",
            "advertools     : 0.10.7\n",
            "pandas         : 1.1.5\n",
            "plotly         : 4.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r__T6OXNtYB1"
      },
      "source": [
        "This is the code to get the national dishes from [Wikipedia's page](https://en.wikipedia.org/wiki/National_dish). This list has a valid criticism (which is also mentioned on the page), about how representative it is. There are countries with populations of over a billion people, who have one or two recipes each, and countries with a population less than ten million who also have their national dish listed. Keep this in mind while thinking about the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6P1NEyi9tYB2"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "page = 'https://en.wikipedia.org/wiki/National_dish'\n",
        "resp = requests.get(page)\n",
        "soup = BeautifulSoup(resp.text, 'lxml')\n",
        "dishlist = [x.text.strip() for x in soup.select('a + b')]\n",
        "dishes_df = pd.DataFrame(dishlist, columns=['dish'])\n",
        "dishes_df.to_csv('recipes_dish_list.csv', index=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "67yK2hYitYB3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "43f973b3-1468-4d66-ea6a-82c8e5b72b49"
      },
      "source": [
        "dishes = pd.read_csv('../input/recipes-search-engine-results-data/recipes_dish_list.csv')\n",
        "dishes.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-38bb5fc63899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdishes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/recipes-search-engine-results-data/recipes_dish_list.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdishes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/recipes-search-engine-results-data/recipes_dish_list.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLR897_ztYB3"
      },
      "source": [
        "To generate the queries, I added \"recipe\" and \"how to make\" to each dish.  \n",
        "As a result each dish will be searched twice, once as \"**dish** recipe\" and once as \"how to make \n",
        "**dish**\".\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Y15bE-FktYB4"
      },
      "source": [
        "queries_recipe = [x + ' recipe' for x in dishes['dish'].tolist()]\n",
        "queries_how_to_make = ['how to make ' + x for x in dishes['dish'].tolist()]\n",
        "queries = queries_recipe + queries_how_to_make\n",
        "queries[:10] + queries[-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00BcVputYB4"
      },
      "source": [
        "To get the data, all you have to run is one line of code for Google and for YouTube. \n",
        "The functions `serp_goog` and `serp_youtube` take a lot of optional arguments to customize your queries. Here we simply give it a list of the queries that we created above. The funtion loops through them all and handles concatenating them into one DataFrame, ready for analysis. If you add other parameters, the function will request data for the **product** of all parameters that you provide.  \n",
        "For example, if you have as queries ['pizza recipe', 'how to make pizza'], then two queries will be generated.  \n",
        "If in addition to `q` you provide a list of countries, let's say ['us', 'ca', 'uk'], then each query will be run three times, once for each country. As a result in this case you will have six requests. You can imagine how this can grow exponentially if you provide a lot of parameters. \n",
        "\n",
        "### Google API Setup\n",
        "\n",
        "1. [Create a custom search engine](https://cse.google.com/cse/all). At first, you might be asked to enter a site to search. Enter any domain, then go to the control panel and remove it. Make sure you enable \"Search the entire web\" and image search. You will also need to get your search engine ID, which you can find on the control panel page.\n",
        "2. [Enable the custom search API](https://console.cloud.google.com/apis/library/customsearch.googleapis.com). The service will allow you to retrieve and display search results from your custom search engine programmatically. You will need to create a project for this first.\n",
        "3. [Create credentials for this project so you can get your key](https://console.developers.google.com/apis/api/customsearch.googleapis.com/credentials).\n",
        "4. [Enable billing for your project](https://console.cloud.google.com/billing/projects) if you want to run more than 100 queries per day. The first 100 queries are free; then for each additional 1,000 queries, you pay USD $5.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ft_Dji5qtYB5"
      },
      "source": [
        "# Code to get the data:\n",
        "cx = 'YOUR_GOOGLE_CUSTOM_SEARCH_ENGINE'\n",
        "key = 'YOUR_GOOGLE_DEVELOPER_API_KEY'\n",
        "\n",
        "# recipes = adv.serp_goog(cx=cx, key=key, q=queries)\n",
        "# recipes_ytb = adv.serp_youtube(key=key, q=queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVlqkZWFtYB6"
      },
      "source": [
        "Here is a sample of the Google data showing the first eight columns, which are the most important. Feel free to explore other columns if you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "GhmZ1Q4itYB7"
      },
      "source": [
        "recipes = pd.read_csv('../input/recipes-search-engine-results-data/recipes_serp_data.csv')\n",
        "print('shape:', recipes.shape)\n",
        "recipes.sample(5).iloc[:, :8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUsQdA-mtYB7"
      },
      "source": [
        "I think column names are clear. Just keep in mind that `queryTime` refers to when this query was requested from Google. This becomes very important if you plan on running the same queries periodically, to show how rankings are changing in time. \n",
        "\n",
        "Now let's create a quick summary of all the domains that appeared, and some summary statistics: \n",
        "\n",
        "- `count`: The number of times the domain appeared in the dataset. Keep in mind that the same domain might appear more than once, in the same query. As you can see below YouTube appeared 154% of the queries (662 ÷ 430 unique queries).\n",
        "- `avg_rank`: The average rank that the domain appeared on, in the dataset. \n",
        "- `coverage`: The number of times the domain appeared ÷ the number of unique queries sent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5RDp9a7MtYB8"
      },
      "source": [
        "summary = (recipes\n",
        "           .groupby(['displayLink'], as_index=False)\n",
        "           .agg({'rank': ['count', 'mean']})\n",
        "           .sort_values(('rank', 'count'), ascending=False)\n",
        "           .assign(coverage=lambda df: df[('rank', 'count')].div(recipes['searchTerms'].nunique())))\n",
        "summary.columns = ['displayLink', 'count', 'avg_rank', 'coverage']\n",
        "summary['displayLink'] = summary['displayLink'].str.replace('www.', '')\n",
        "summary['avg_rank'] = summary['avg_rank'].round(1)\n",
        "summary['coverage'] = summary['coverage'].mul(100).round(1).astype(str).add('%')\n",
        "summary.head(20).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilx3_v6HtYB8"
      },
      "source": [
        "YouTube is clearly the undisputed leader, and very far from the first competitor. \n",
        "Let's visualize this summary for a different view. Before that, we create a summary showing how many times each domain appeared on every rank.  \n",
        "\n",
        "For example, in the sample below, cooking.nytimes.com appeared on rank one, eight times, on rank two, five times, and so on. Before that I created `top_domains` which are the top ten domains that appeared the most. You can change this of course if you want more, or fewer domains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AgUBHThltYB9"
      },
      "source": [
        "top_domains = recipes['displayLink'].value_counts()[:10].index.tolist()\n",
        "top_df = recipes[recipes['displayLink'].isin(top_domains)]\n",
        "\n",
        "rank_counts = top_df.groupby(['displayLink', 'rank']).agg({'rank': ['count']}).reset_index()\n",
        "rank_counts.columns = ['displayLink', 'rank', 'count']\n",
        "rank_counts.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7mmIE6sWtYB9"
      },
      "source": [
        "fig = go.FigureWidget()\n",
        "\n",
        "fig.add_scatter(x=top_df['displayLink'].str.replace('www.', ''),\n",
        "                y=top_df['rank'], mode='markers',\n",
        "                marker={'size': 35, 'opacity': 0.035,})\n",
        "\n",
        "fig.add_scatter(x=rank_counts['displayLink'].str.replace('www.', ''),\n",
        "                y=rank_counts['rank'], mode='text', text=rank_counts['count'])\n",
        "\n",
        "fig.layout.hovermode = False\n",
        "fig.layout.yaxis.autorange = 'reversed'\n",
        "fig.layout.yaxis.zeroline = False\n",
        "fig.layout.yaxis.tickvals = list(range(1, 11))\n",
        "fig.layout.height = 600\n",
        "fig.layout.title = 'Top Domains for 430 Recipes Keywords - Google'\n",
        "fig.layout.yaxis.title = 'SERP Rank (number of appearances)'\n",
        "fig.layout.showlegend = False\n",
        "fig.layout.paper_bgcolor = '#eeeeee'\n",
        "fig.layout.plot_bgcolor = '#eeeeee'\n",
        "iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEATogBtYB9"
      },
      "source": [
        "For every appearance on a search result, an almost transparent circle is plotted on that position for the domain. The more times it appears, the more opaque the circle gets. As you can expect, YouTube's results clearly stand out from the rest.  \n",
        "The numbers on the circles show the exact number of times that each domain appeared on that position (rank). YouTube appears mostly on ranks three to five, as shown above, in the summary table where they have an average rank of 4.8.\n",
        "\n",
        "### Comparison with flights keywords \n",
        "\n",
        "To get an idea of how this distribution compares to other industries, here is a quick look at a similar data set for flights and tickets keywords. This is for one hundred top destinations (cities), and two keyword variations for each.  \n",
        "The below code is pretty much the same as the one we ran for recipes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bKhR_VXPtYB-"
      },
      "source": [
        "flights_serp = pd.read_csv('../input/search-engine-results-flights-tickets-keywords/flights_tickets_serp2018-12-16.csv')\n",
        "\n",
        "flights_serp = flights_serp[flights_serp['gl'] == 'us']\n",
        "flights_serp.iloc[:, :8].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SYwNQquTtYB-"
      },
      "source": [
        "summary_flights = (flights_serp\n",
        "                   .groupby(['displayLink'], as_index=False)\n",
        "                   .agg({'rank': ['count', 'mean']})\n",
        "                   .sort_values(('rank', 'count'), ascending=False)\n",
        "                   .assign(coverage=lambda df: df[('rank', 'count')].div(flights_serp['searchTerms'].nunique())))\n",
        "\n",
        "summary_flights.columns = ['displayLink', 'count', 'avg_rank', 'coverage']\n",
        "summary_flights['displayLink'] = summary_flights['displayLink'].str.replace('www.', '')\n",
        "summary_flights['avg_rank'] = summary_flights['avg_rank'].round(1)\n",
        "summary_flights['coverage'] = summary_flights['coverage'].mul(100).round(1).astype(str).add('%')\n",
        "summary_flights.head(10).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA9ORZZgtYB_"
      },
      "source": [
        "This looks quite different from how recipes domains are distributed on search. YouTbe has coverage of 154%, and the first competitor is at 41%. Here, there is fierce competition among the top four, all of which are almost 80% and above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iArssR1utYB_"
      },
      "source": [
        "top_domains_flights = flights_serp['displayLink'].value_counts()[:10].index.tolist()\n",
        "top_df_flights = flights_serp[flights_serp['displayLink'].isin(top_domains_flights)]\n",
        "\n",
        "rank_counts_flights = top_df_flights.groupby(['displayLink', 'rank']).agg({'rank': ['count']}).reset_index()\n",
        "rank_counts_flights.columns = ['displayLink', 'rank', 'count']\n",
        "rank_counts_flights.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jUsLVER9tYCA"
      },
      "source": [
        "fig = go.FigureWidget()\n",
        "\n",
        "fig.add_scatter(x=top_df_flights['displayLink'].str.replace('www.', ''),\n",
        "                y=top_df_flights['rank'], mode='markers',\n",
        "                marker={'size': 35, 'opacity': 0.035,})\n",
        "\n",
        "\n",
        "fig.add_scatter(x=rank_counts_flights['displayLink'].str.replace('www.', ''),\n",
        "                y=rank_counts_flights['rank'], mode='text', text=rank_counts_flights['count'])\n",
        "\n",
        "fig.layout.hovermode = False\n",
        "fig.layout.yaxis.autorange = 'reversed'\n",
        "fig.layout.yaxis.zeroline = False\n",
        "fig.layout.yaxis.tickvals = list(range(1, 11))\n",
        "fig.layout.height = 600\n",
        "fig.layout.title = 'Top Domains for Flights and Tickets Keywords - Google - USA'\n",
        "fig.layout.yaxis.title = 'SERP Rank (number of appearances)'\n",
        "fig.layout.showlegend = False\n",
        "fig.layout.paper_bgcolor = '#eeeeee'\n",
        "fig.layout.plot_bgcolor = '#eeeeee'\n",
        "iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6JGt4mhtYCA"
      },
      "source": [
        "## Recipe Keywords on YouTube"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l9ZEv4vStYCA"
      },
      "source": [
        "recipes_ytb = pd.read_csv('../input/recipes-search-engine-results-data/recipes_serp_youtube_data.csv')\n",
        "print('shape:', recipes_ytb.shape)\n",
        "recipes_ytb.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0lV6QAstYCA"
      },
      "source": [
        "Note the difference in the number of columns here. Ninety compared to twenty seven on Google search.  \n",
        "There are several numeric columns that we can use to analyze YouTube data. Here they are (I excluded `video.favoriteCount` and `channel.commentCount` because they are all zeros in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YipiLrL5tYCB"
      },
      "source": [
        "count_columns = [col for col in recipes_ytb.columns if 'Count' in col if col not in ['video.favoriteCount', 'channel.commentCount']]\n",
        "count_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEKIA0iKtYCB"
      },
      "source": [
        "I wanted to quickly check if there is any correlation between any of those numeric columns and the rank of the channel. Apparently there seems to be none!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RrCp0-S0tYCB"
      },
      "source": [
        "for col in count_columns:\n",
        "    corr_df = recipes_ytb[['rank', col]].corr()\n",
        "    print(f\"{col:>30} {corr_df['rank'][-1]:%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3dh9aHA9tYCB"
      },
      "source": [
        "summary_ytb = (recipes_ytb\n",
        "              .groupby(['channelTitle'], as_index=False)\n",
        "              .agg({'rank': ['count', 'mean']})\n",
        "              .sort_values(('rank', 'count'), ascending=False)\n",
        "              .assign(coverage=lambda df: df[('rank', 'count')].div(recipes_ytb['q'].nunique())))\n",
        "summary_ytb.columns = ['channelTitle', 'count', 'avg_rank', 'coverage']\n",
        "# summary_ytb['displayLink'] = summary_ytb['displayLink'].str.replace('www.', '')\n",
        "summary_ytb['avg_rank'] = summary_ytb['avg_rank'].round(1)\n",
        "summary_ytb['coverage'] = summary_ytb['coverage'].mul(100).round(1).astype(str).add('%')\n",
        "summary_ytb.head(10).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYBYEMhKtYCC"
      },
      "source": [
        "Here the picture is completely different from Google search (for both, recipes and flights). The top channel has a coverage score of 4.6%.  \n",
        "Remember that by default YouTube returns five results per query, so this also has an effect on how data are distributed. Feel free to experiment with more results if you want. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "D3xGQ9FItYCC"
      },
      "source": [
        "top_channels = recipes_ytb['channelTitle'].value_counts()[:10].index.tolist()\n",
        "top_channels_df = recipes_ytb[recipes_ytb['channelTitle'].isin(top_channels)]\n",
        "\n",
        "rank_counts_ytb = top_channels_df.groupby(['channelTitle', 'rank']).agg({'rank': ['count']}).reset_index()\n",
        "rank_counts_ytb.columns = ['channelTitle', 'rank', 'count']\n",
        "rank_counts_ytb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Up7Ait9ntYCD"
      },
      "source": [
        "fig = go.FigureWidget()\n",
        "\n",
        "fig.add_scatter(x=top_channels_df['channelTitle'],\n",
        "                y=top_channels_df['rank'], mode='markers',\n",
        "                marker={'size': 35, 'opacity': 0.25,})\n",
        "\n",
        "\n",
        "fig.add_scatter(x=rank_counts_ytb['channelTitle'].str.replace('www.', ''),\n",
        "                y=rank_counts_ytb['rank'], mode='text', text=rank_counts_ytb['count'])\n",
        "\n",
        "fig.layout.hovermode = False\n",
        "fig.layout.yaxis.autorange = 'reversed'\n",
        "fig.layout.yaxis.zeroline = False\n",
        "fig.layout.yaxis.tickvals = list(range(1, 11))\n",
        "fig.layout.height = 600\n",
        "fig.layout.title = 'Recipes Keywords Rankings - YouTube'\n",
        "fig.layout.yaxis.title = 'SERP Rank (number of appearances)'\n",
        "fig.layout.showlegend = False\n",
        "fig.layout.paper_bgcolor = '#eeeeee'\n",
        "fig.layout.plot_bgcolor = '#eeeeee'\n",
        "iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8JDkBS8tYCD"
      },
      "source": [
        "## Text Analysis\n",
        "\n",
        "Since we have a lot of numeric columns that describe videos and channels, and since we also have many text columns, let's do some word counting and see which are the most common.  \n",
        "Note that there are two types of counting that are provided by the `word_frequency` function. \n",
        "\n",
        "- Absolute (`abs_freq`): This is a simple count of how many times each word was used in the text list that we specify. \n",
        "- Weighted (`wtd_freq`): This takes into consideration the numeric value that we provide. For example, if a video has the title \"sushi recipe\" and that video got one thousand views, then \"sushi\" and \"recipe\" would be counted a thousand times each. This shows how important the word was from a consumption perspective and not just from a production point of view. I have another article on [text analysis for online marketers](https://www.semrush.com/blog/text-analysis-for-online-marketers/) if you are interested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AHN-prUWtYCD"
      },
      "source": [
        "serp_word_freq = adv.word_frequency(recipes['title'], \n",
        "                                    rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        "serp_word_freq.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ihsjgktYCE"
      },
      "source": [
        "The above table shows the most used words in the title column of the `recipes` DataFrame, for Google. There is no numeric value (other than the rank) so, without providing a `num_list` argument, the frequency defaults to one.  \n",
        "\"recipe\", \"food\", \"youtube\", and \"recipes\" (the top words) are not surprising. But for me, having \"chicken\" as the top specific food word is interesting. It is almost twice as much as beef. I wouldn't expect rice to be that prominent either. What do you expect? \n",
        "\n",
        "Below is the same count for the snippet column. Again chicken, together with rice, are the only specific foods mentioned. The rest are generic words about recipes and food. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nm_nRqH6tYCE"
      },
      "source": [
        "serp_word_freq_snippet = adv.word_frequency(recipes['snippet'].fillna(''), rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        "serp_word_freq_snippet.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXHehRgMtYCE"
      },
      "source": [
        "Things get more interesting with weighted frequency.  \n",
        "Here is the word frequency for YouTube's DataFrame, counting words in the title column, with videos' number of views."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Uc4mEoUItYCF"
      },
      "source": [
        "serp_ytb_word_freq_title = adv.word_frequency(text_list=recipes_ytb['title'].fillna(''), \n",
        "                                              num_list=recipes_ytb['video.viewCount'],\n",
        "                                              rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        "serp_ytb_word_freq_title.head(10).style.format({'wtd_freq': '{:,.0f}', 'rel_value': '{:,.0f}'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUCsZSPQtYCF"
      },
      "source": [
        "The DataFrame is sorted by `wtd_freq` by default but you can obviously change that if you want.  \n",
        "\"recipe\" was used 412 times, and the sum total of views for all videos who's title contains \"recipes\" was 394,834,312.  \n",
        "The words on indexes two to six are interesting. They all have an `abs_freq` of one, and the same number on `wtd_freq`. This means that there was one video who's title contained these words, and this is further confirmed by the fact that they all have the same `wtd_freq`.  \n",
        "This is a very important example, where one row can affect the whole dataset. This is typical on social media where you have people with tens of millions of followers, and with one post, can make such a dramatic difference to the averages.  \n",
        "Again \"chicken\" is the top specific food word in this DataFrame. \n",
        "\n",
        "Let's run the same function for the description column (also weighted by the number of views):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uLJmC3WdtYCF"
      },
      "source": [
        "serp_ytb_word_freq_desc = adv.word_frequency(recipes_ytb['description'].fillna(''), recipes_ytb['video.viewCount'],\n",
        "                                              rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        "serp_ytb_word_freq_desc.head(20).style.format({'wtd_freq': '{:,.0f}', 'rel_value': '{:,.0f}'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5GM9NoMtYCF"
      },
      "source": [
        "#### Video tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rLNS04CztYCG"
      },
      "source": [
        "serp_ytb_word_freq_tags = adv.word_frequency(recipes_ytb['video.tags'].fillna(''), recipes_ytb['video.viewCount'],\n",
        "                                              rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        "serp_ytb_word_freq_tags.head(20).style.format({'wtd_freq': '{:,.0f}', 'rel_value': '{:,.0f}'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CUnc1wwtYCG"
      },
      "source": [
        "#### Video Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "16qsuCbXtYCK"
      },
      "source": [
        "serp_ytb_word_freq_topics = adv.word_frequency(recipes_ytb['video.topicCategories'].fillna(''), recipes_ytb['video.viewCount'],\n",
        "                                              rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        "serp_ytb_word_freq_topics.head(20).style.format({'wtd_freq': '{:,.0f}', 'rel_value': '{:,.0f}'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQt-_PH0tYCK"
      },
      "source": [
        "Another option that is available through the `word_frequency` function is that you can provide a regular expression to extract specific patterns of words.  \n",
        "Let's say you want to know which/if hashtags are popular in video descriptions. All you have to do is supply the regex for that and the function will extract them, and only run the counts for those extracted words.  \n",
        "The `regex` module in `advertools` provides a set of regexes that can be used.  \n",
        "Here we use `adv.regex.HASHTAG_RAW` to extract the hashtags. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "af8sG7c3tYCK"
      },
      "source": [
        "(adv.word_frequency(recipes_ytb['video.description'].fillna(''), \n",
        "                    recipes_ytb['video.viewCount'],\n",
        "                    regex=adv.regex.HASHTAG_RAW,\n",
        "                    rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        " .head(15)\n",
        " .style.format({'wtd_freq': '{:,.0f}', 'rel_value': '{:,.0f}'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDVo35_rtYCL"
      },
      "source": [
        "### @mentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OQ5oORQdtYCL"
      },
      "source": [
        "(adv.word_frequency(recipes_ytb['video.description'].fillna(''), \n",
        "                    recipes_ytb['video.viewCount'],\n",
        "                    regex=adv.regex.MENTION_RAW,\n",
        "                    rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        " .head(15)\n",
        " .style.format({'wtd_freq': '{:,.0f}', 'rel_value': '{:,.0f}'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXn7PV3LtYCL"
      },
      "source": [
        "### Emoji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "px8mbU_1tYCL"
      },
      "source": [
        "(adv.word_frequency(recipes_ytb['video.description'].fillna(''), \n",
        "                   recipes_ytb['video.viewCount'],\n",
        "                   regex=adv.emoji_dict.emoji_regexp,\n",
        "                   rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        " .head(15)\n",
        " .assign(emoji_text=lambda df: [adv.emoji_dict.emoji_dict[x] for x in df['word']])\n",
        " .style.format({'wtd_freq': '{:,.0f}', 'rel_value': '{:,.0f}'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4P3AyzLtYCM"
      },
      "source": [
        "### Currency Symbols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mDyqH59BtYCM"
      },
      "source": [
        "(adv.word_frequency(recipes_ytb['video.description'].fillna(''), \n",
        "                    recipes_ytb['video.viewCount'],\n",
        "                    regex=adv.regex.CURRENCY_RAW,\n",
        "                    rm_words=list(adv.stopwords['english']) + ['-', '', '|', '–', '&'])\n",
        " .head(15)\n",
        " .style.format({'wtd_freq': '{:,.0f}', 'rel_value': '{:,.0f}'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jum9v3XgtYCM"
      },
      "source": [
        "This was a quick overview of doing some SEO research on Google and YouTube, and how you can compare, and extract certain information.  \n",
        "There is more that can be done but these are some of the main features that I think are important.  \n",
        "If you have any suggestions/questions, or come across any bugs, [let me know.](https://github.com/eliasdabbas/advertools/issues)"
      ]
    }
  ]
}